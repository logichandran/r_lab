def edit_min_dis(s1, s2):
  n = len(s1)
  m = len(s2)

  dp = [[0 for _ in range(n+1)] for _ in range(m+1)]

  for j in range(m+1):
    dp[0][j] = j
  
  for i in range(n+1):
    dp[i][0] = i

  for i in range(1,n+1):
    for j in range(1,m+1):
      if s1[i-1] == s2[j-1]:
        dp[i][j] = dp[i-1][j-1]
      else:
        dp[i][j] = 1 + min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])
  
  return dp[n][m]

print(edit_min_dis("intention","execution"))
-------------------------------------------------------------------------------------------------------------------------------

def stemming(s):
  if s.endswith('ing'):
    return s[0:-3]
  if s.endswith('ly') or s.endswith('ed'):
    return s[0:-2]
  return s

def lemming(s):
  lemmas = {
      'programming' : 'program' ,
      'loving' : 'Love' ,
      'lovely' : 'Love' ,
      'kind' : 'kind'
  }
  return lemmas.get(s.lower(),s)

for word in ['programming','loving','lovely','kind']:
  print(lemming(word))
  print(stemming(word))

-------------------------------------------------------------------------------------------------------------------------------
import re

text = """
Emails: sunil123@gmail.com, shyam.office@college.edu, contact_me@domain.org
Phones: 9876543210, 9123456789
"""
names = ["Sunil", "Shyam", "Ankit", "Surjeet", "Sumit", "Subhi", "Surbhi", "Siddharth", "Sujan"]
abc = "a ab abc abcc abccc abb abcd"

email_pattern = r'\b[\w\.-]+@[\w\.-]+\.\w{2,4}\b'
phone_pattern = r'\b\d{10}\b'
su_pattern = r'\bSu\w{3}\b'
ab_pattern = r'ab(?:c)*'       # ab + 0 or more c's
a_pattern = r'a(?:bc)*'        # a + 0 or more 'bc'
an_pattern = r'ab(?:c)?'       # ab + 0 or 1 c


print(re.findall(email_pattern,text))
print(re.findall(phone_pattern,text))
print(re.findall(su_pattern,str(names)))
print(re.findall(ab_pattern,abc))
print(re.findall(a_pattern,abc))
print(re.findall(an_pattern,abc))

found = re.search(phone_pattern,text)
print(found.group() if found else "not found")

matched = re.match(r'\nEmails' , text)
print(matched.group() if matched else "not found")

replaced = re.sub(phone_pattern , "xxxxxxxxxx" , text)
print(replaced)

email_complied = re.compile(email_pattern)
print(email_complied.findall(text))

---------------------------------------------------------------------------------------------------------------------------------------------
from collections import defaultdict
corpus = [
    "<s> I am Henry </s>",
    "<s> I like college </s>",
    "<s> Do Henry like college </s>",
    "<s> Henry I am </s>",
    "<s> Do I like Henry </s>",
    "<s> Do I like college </s>",
    "<s> I do like Henry </s>"
]

trigram_counts = defaultdict(int)

for sentence in corpus:
  words = sentence.replace('<s>' , "").replace('</s>','').strip().split()
  for i in range(len(words)-2):
    trigram = (words[i],words[i+1],words[i+2])
    trigram_counts[trigram] += 1

next_word_count = defaultdict(int)
for sentence in corpus:
  words = sentence.replace('<s>' , "").replace('</s>','').strip().split()
  for i in range(len(words)-3):
    if words[i] == "Do" and words[i+1] == "I" and words[i+2] == "like":
      next = words[i+3]
      next_word_count[next] += 1

print(max(next_word_count, key=next_word_count.get))
---------------------------------------------------------------------------------------------------
sentences = [
    "I need a flight from Atlanta.",
    "Everything to permit us.",
    "I would like to address the public on this issue.",
    "We need your shipping address."
]


for sentence in sentences:
  words = sentence.strip().split()
  for word in words:
    w = word.lower()
    if w in ['i' ,'us','we','your']:
      print(f'{w} (PRP) ' , end=" ")
    elif w in ['need' , 'permit' , 'would', 'like','shipping']:
      print(f"{w} (VBP) " , end= " ")
    elif w in ['a','the', 'this']:
      print(f'{w}  (DT) ', end= " ")
    elif w in ["to"]:
      print(f'{w} (TO) ', end= " ")
    elif w in ['flight' , 'atlanta' , 'everything' ,'address','public','issue']:
      print(f'{w} (NN) ' , end = " ")
    elif w in ['from' , 'in' , 'on']:
      print(f'{w} (IN) ' , end = " ")
  print()

------------------------------------------------------------------------------------------------------------------
text = """
Artificial Intelligence (AI) has significantly transformed how we interact with technology.
From voice assistants like Siri and Alexa to recommendation systems on Netflix and YouTube, AI is all around us.
It enables machines to learn from data and improve over time without being explicitly programmed.
In healthcare, AI is helping doctors diagnose diseases more accurately and quickly.
In transportation, self-driving cars use AI to navigate and avoid obstacles.
Businesses use AI to automate repetitive tasks and gain insights from large datasets.
In education, AI-powered tutors assist students with personalized learning.
However, AI also poses ethical challenges such as job displacement, data privacy, and algorithmic bias.
As AI continues to evolve, it's important for society to strike a balance between innovation and regulation.
Responsible development and use of AI will shape the future of technology and humanity.
"""

import re
from collections import Counter

# Clean and split text into sentences
sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]

# Tokenize words
words = re.findall(r'\b\w+\b', text.lower())
stopwords = set(["the", "is", "has", "a", "and", "to", "of", "in", "with", "for", "on", "as", "it", "we", "how", "from", "be", "are", "by", "this", "that", "such", "will"])

# Filter stopwords
filtered_words = [word for word in words if word not in stopwords]

# Word frequency
word_freq = Counter(filtered_words)

# Score sentences
sentence_scores = {}
for sent in sentences:
    score = 0
    for word in re.findall(r'\b\w+\b', sent.lower()):
        if word in word_freq:
            score += word_freq[word]
    sentence_scores[sent] = score

# Top 3 sentences
top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]
extractive_summary = '. '.join(top_sentences) + '.'

print("ðŸ“Œ Extractive Summary:\n", extractive_summary)

-----------------------------------------------------------------------------------------------------------------

